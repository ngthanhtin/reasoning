{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11c234c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch & misc\n",
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# number of attributes and landmark annotations\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from datasets.cub200 import CUB200\n",
    "from models.model_factory import model_generator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# dataset_root='../data/'\n",
    "dataset_root = '/home/tin/datasets/cub/dataset/CUB/'\n",
    "DEVICE = 'cuda:5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22a86d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 200\n",
    "num_landmarks = 15\n",
    "\n",
    "\n",
    "def cut_borders(inputs, masks, landmarks, bbox):\n",
    "    # row and col check, x means row num here\n",
    "    height = inputs.shape[2]\n",
    "    width = inputs.shape[3]\n",
    "\n",
    "    # calculate border width and height\n",
    "    border_h = height % 16\n",
    "    border_w = width % 16\n",
    "\n",
    "    # calculate the new shape\n",
    "    new_height = height - border_h\n",
    "    new_width = width - border_w\n",
    "\n",
    "    # cut the inputs\n",
    "    inputs_cropped = inputs[:, :, (border_h // 2):new_height + (border_h // 2), (border_w // 2):new_width + (border_w // 2)]\n",
    "    masks_cropped = masks[:, :, (border_h // 2):new_height + (border_h // 2), (border_w // 2):new_width + (border_w // 2)]\n",
    "\n",
    "    # transform the landmarks correspondingly\n",
    "    landmarks_cropped = landmarks.squeeze(0)\n",
    "    for i in range(num_landmarks):\n",
    "        if abs(landmarks_cropped[i][-1]) > 1e-5:\n",
    "            landmarks_cropped[i][-3] -= (border_w // 2)  # column shift\n",
    "            landmarks_cropped[i][-2] -= (border_h // 2)  # row shift\n",
    "\n",
    "            # remove the landmarks if unlucky (never really stepped in for test set of cub200)\n",
    "            if landmarks_cropped[i][-3] < 0 or landmarks_cropped[i][-3] >= new_width or \\\n",
    "                    landmarks_cropped[i][-2] < 0 or landmarks_cropped[i][-2] >= new_height:\n",
    "                landmarks_cropped[i][-3] = 0\n",
    "                landmarks_cropped[i][-2] = 0\n",
    "                landmarks_cropped[i][-1] = 0\n",
    "\n",
    "    landmarks_cropped = landmarks_cropped.unsqueeze(0)\n",
    "\n",
    "    # transform the bounding box correspondingly\n",
    "    bbox_cropped = bbox\n",
    "    bbox_cropped[0, 1] = bbox_cropped[0, 1] - (border_w // 2)\n",
    "    bbox_cropped[0, 2] = bbox_cropped[0, 2] - (border_h // 2)\n",
    "    bbox_cropped[0, 1].clamp_(min=0)\n",
    "    bbox_cropped[0, 2].clamp_(min=0)\n",
    "    bbox_cropped[0, 3].clamp_(max=new_width)\n",
    "    bbox_cropped[0, 4].clamp_(max=new_height)\n",
    "\n",
    "    return inputs_cropped, masks_cropped, landmarks_cropped, bbox_cropped\n",
    "\n",
    "\n",
    "def calc_center(assignment, num_parts):\n",
    "    # row and col check, x means row num here\n",
    "    batch_size = assignment.shape[0]\n",
    "    nparts = assignment.shape[1]\n",
    "    height = assignment.shape[2]\n",
    "    width = assignment.shape[3]\n",
    "\n",
    "    # assertions\n",
    "    assert nparts == num_parts\n",
    "\n",
    "    # generate the location map\n",
    "    col_map = torch.arange(1, width + 1).view(1, 1, 1, width).expand(batch_size, nparts, height, width).float().to(DEVICE)\n",
    "    row_map = torch.arange(1, height + 1).view(1, 1, height, 1).expand(batch_size, nparts, height, width).float().to(DEVICE)\n",
    "\n",
    "    # multiply the location map with the soft assignment map\n",
    "    col_weighted = (col_map * assignment).view(batch_size, nparts, -1)\n",
    "    row_weighted = (row_map * assignment).view(batch_size, nparts, -1)\n",
    "\n",
    "    # sum of assignment as the denominator\n",
    "    denominator = torch.sum(assignment.view(batch_size, nparts, -1), dim=2) + 1e-8\n",
    "\n",
    "    # calculate the weighted average of location maps as centers\n",
    "    col_mean = torch.sum(col_weighted, dim=2) / denominator\n",
    "    row_mean = torch.sum(row_weighted, dim=2) / denominator\n",
    "\n",
    "    # prepare the centers for return\n",
    "    col_centers = col_mean.unsqueeze(2)\n",
    "    row_centers = row_mean.unsqueeze(2)\n",
    "\n",
    "    # N * K * 1 -> N * K * 2 -> N * (K * 2)\n",
    "    centers = torch.cat([col_centers, row_centers], dim=2).view(batch_size, nparts * 2)\n",
    "\n",
    "    # # upsample to the image resolution (256, 256)\n",
    "    # centers = centers\n",
    "\n",
    "    return centers\n",
    "\n",
    "\n",
    "def create_centers(data_loader, model, num_parts, is_scops):\n",
    "    # tensor for collecting centers, labels, existence masks\n",
    "    centers_collection = []\n",
    "    annos_collection = []\n",
    "    masks_collection = []\n",
    "\n",
    "    # iterating the data loader, landmarks shape: [N, num_landmarks, 4], column first\n",
    "    # bbox shape: [N, 5]\n",
    "    for i, (input_raw, fg_mask_raw, _, landmarks_raw, bbox_raw) in enumerate(tqdm(data_loader)):\n",
    "\n",
    "        # to device\n",
    "        input_raw = input_raw.to(DEVICE)\n",
    "        landmarks_raw = landmarks_raw.to(DEVICE)\n",
    "        bbox_raw = bbox_raw.to(DEVICE)\n",
    "        fg_mask_raw = fg_mask_raw.to(DEVICE)\n",
    "\n",
    "        # cut the input and transform the landmark\n",
    "        inputs, fg_masks, landmarks_full, bbox = cut_borders(input_raw, fg_mask_raw, landmarks_raw, bbox_raw)\n",
    "\n",
    "        # gather the landmark annotations, center outputs and existence masks\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # generate assignment map\n",
    "            if is_scops:\n",
    "                assignment = torch.softmax(F.interpolate(model(inputs)[2], size=inputs.shape[-2:], mode='bilinear'), dim=1)[:, 1:]\n",
    "            else:\n",
    "                assignment = torch.softmax(F.interpolate(model(inputs)[2], size=inputs.shape[-2:], mode='bilinear'), dim=1) * fg_masks\n",
    "\n",
    "            # calculate the center coordinates of shape [N, num_parts, 2]\n",
    "            centers = calc_center(assignment, num_parts)\n",
    "            centers = centers.contiguous().view(centers.shape[0], num_parts, 2)\n",
    "\n",
    "            # extract the landmark and existence mask, [N, num_landmarks, 2]\n",
    "            landmarks = landmarks_full[:, :, -3:-1]\n",
    "            masks = landmarks_full[:, :, -1].unsqueeze(2).expand_as(landmarks)\n",
    "\n",
    "            # normalize the coordinates with the bounding boxes\n",
    "            bbox = bbox.unsqueeze(2)\n",
    "            centers[:, :, 0] = (centers[:, :, 0] - bbox[:, 1]) / bbox[:, 3]\n",
    "            centers[:, :, 1] = (centers[:, :, 1] - bbox[:, 2]) / bbox[:, 4]\n",
    "            landmarks[:, :, 0] = (landmarks[:, :, 0] - bbox[:, 1]) / bbox[:, 3]\n",
    "            landmarks[:, :, 1] = (landmarks[:, :, 1] - bbox[:, 2]) / bbox[:, 4]\n",
    "\n",
    "            # collect the centers, annotations and masks\n",
    "            centers_collection.append(centers)\n",
    "            annos_collection.append(landmarks)\n",
    "            masks_collection.append(masks)\n",
    "\n",
    "    # list into tensors\n",
    "    centers_tensor = torch.cat(centers_collection, dim=0)\n",
    "    annos_tensor = torch.cat(annos_collection, dim=0)\n",
    "    masks_tensor = torch.cat(masks_collection, dim=0)\n",
    "\n",
    "    # reshape the tensors\n",
    "    centers_tensor = centers_tensor.contiguous().view(centers_tensor.shape[0], num_parts * 2)\n",
    "    annos_tensor = annos_tensor.contiguous().view(centers_tensor.shape[0], num_landmarks * 2)\n",
    "    masks_tensor = masks_tensor.contiguous().view(centers_tensor.shape[0], num_landmarks * 2)\n",
    "\n",
    "    return centers_tensor, annos_tensor, masks_tensor\n",
    "\n",
    "\n",
    "def L2_distance(prediction, annotation):\n",
    "    diff_sq = (prediction - annotation) * (prediction - annotation)\n",
    "    L2_dists = np.sqrt(np.sum(diff_sq, axis=2))\n",
    "    error = np.mean(L2_dists)\n",
    "    return error\n",
    "\n",
    "\n",
    "class To_tensor():\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img, np.float32, copy=False)\n",
    "        if len(img.shape) == 2:\n",
    "            img = np.expand_dims(img, 2)\n",
    "        img = torch.from_numpy(img)\n",
    "        img = img.permute((2, 0, 1)).contiguous()\n",
    "        return img\n",
    "\n",
    "\n",
    "def get_nmi_inputs(data_loader, model, num_parts, is_scops, visualize=False):\n",
    "\n",
    "    all_nmi_preds = []\n",
    "    all_nmi_preds_w_bg = []\n",
    "    all_nmi_gts = []\n",
    "\n",
    "    # iterating the data loader, landmarks shape: [N, num_landmarks, 4], column first\n",
    "    # bbox shape: [N, 5]\n",
    "    part_label_dict = ['back', 'beak', 'belly', 'breast', 'crown', 'forehead', 'left eye', 'left leg', 'left wing', 'nape', 'right eye', 'right leg', 'right wing', 'tail', 'throat']\n",
    "    colors_dict = ['b', 'g', 'r', 'c', 'm',\n",
    "                   'y', 'w', 'tab:orange', 'tab:purple', 'tab:brown',\n",
    "                   'tab:pink', 'tab:gray', 'lime', 'aqua', 'fuchsia']\n",
    "    save_segment_kp_dict = {}\n",
    "    for i, (input_raw, fg_mask_raw, _, landmarks_raw, bbox_raw, image_path) in enumerate(tqdm(data_loader)):\n",
    "        save_segment_kp_dict[image_path[0]] = {}\n",
    "\n",
    "        # to device\n",
    "        input_raw = input_raw.to(DEVICE)\n",
    "        landmarks_raw = landmarks_raw.to(DEVICE)\n",
    "        bbox_raw = bbox_raw.to(DEVICE)\n",
    "        fg_mask_raw = fg_mask_raw.to(DEVICE)\n",
    "\n",
    "        # cut the input and transform the landmark\n",
    "        inputs, fg_masks, landmarks_full, bbox = cut_borders(input_raw, fg_mask_raw, landmarks_raw, bbox_raw)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # generate assignment map\n",
    "            if is_scops:\n",
    "                part_name_mat = F.interpolate(model(inputs)[2], size=inputs.shape[-2:], mode='bilinear', align_corners=False)[:, 1:]\n",
    "                part_name_mat_w_bg = F.interpolate(model(inputs)[2], size=inputs.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                part_name_mat = F.interpolate(model(inputs)[2], size=inputs.shape[-2:], mode='bilinear', align_corners=False)\n",
    "                part_name_mat_w_bg = torch.cat([1-fg_masks,\n",
    "                                                fg_masks*torch.softmax(F.interpolate(model(inputs)[2], size=inputs.shape[-2:], mode='bilinear', align_corners=False), dim=1)\n",
    "                                                ], dim=1)\n",
    "            if i % 1 == 0:\n",
    "                if visualize:\n",
    "                    plt.imshow(inputs.permute(0,2,3,1).cpu().numpy().squeeze())\n",
    "                    plt.show()\n",
    "                    plt.imshow(fg_masks.cpu().numpy().squeeze()*50)\n",
    "                    plt.show()\n",
    "                    plt.imshow(part_name_mat_w_bg.argmax(1).cpu().numpy().squeeze())\n",
    "        \n",
    "                xs = landmarks_full[0, :, 1].detach().cpu()\n",
    "                ys = landmarks_full[0, :, 2].detach().cpu()\n",
    "                visible = (landmarks_full[0, :, 3] > 0.5).detach().cpu()\n",
    "                for i, (x, y, is_visible) in enumerate(zip(xs, ys, visible)):\n",
    "                    if is_visible:\n",
    "                        label = f\"{part_label_dict[i]} (o)\"\n",
    "                        save_segment_kp_dict[image_path[0]][part_label_dict[i]] = [x.item(),y.item()]\n",
    "                    else:\n",
    "                        label = f\"{part_label_dict[i]} (x)\"\n",
    "                        save_segment_kp_dict[image_path[0]][part_label_dict[i]] = [-1, -1]\n",
    "                    if visualize:\n",
    "                        plt.scatter(x, y, label=label, s=200, edgecolors='black', c=colors_dict[i])\n",
    "                \n",
    "                if visualize:\n",
    "                    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "                    plt.tight_layout()\n",
    "                    # plt.scatter(landmarks_full[0, :, 1].detach().cpu(), landmarks_full[0, :, 2].detach().cpu(), color='red')\n",
    "                    plt.show()\n",
    "\n",
    "                \n",
    "\n",
    "            # extract the landmark and existence mask, [N, num_landmarks, 2]\n",
    "            visible = landmarks_full[:, :, 3] > 0.5\n",
    "            points = landmarks_full[:, :, 1:3].unsqueeze(2).clone()\n",
    "\n",
    "            points[:, :, :, 0] /= inputs.shape[-1]  # W\n",
    "            points[:, :, :, 1] /= inputs.shape[-2]  # H\n",
    "            assert points.min() > -1e-7 and points.max() < 1+1e-7\n",
    "            points = points * 2 - 1\n",
    "\n",
    "            pred_parts_loc = F.grid_sample(part_name_mat.float(), points, mode='nearest', align_corners=False)\n",
    "            pred_parts_loc = torch.argmax(pred_parts_loc, dim=1).squeeze(2)\n",
    "            pred_parts_loc = pred_parts_loc[visible]\n",
    "            all_nmi_preds.append(pred_parts_loc.cpu().numpy())\n",
    "\n",
    "            pred_parts_loc_w_bg = F.grid_sample(part_name_mat_w_bg.float(), points, mode='nearest', align_corners=False)\n",
    "            pred_parts_loc_w_bg = torch.argmax(pred_parts_loc_w_bg, dim=1).squeeze(2)\n",
    "            pred_parts_loc_w_bg = pred_parts_loc_w_bg[visible]\n",
    "            all_nmi_preds_w_bg.append(pred_parts_loc_w_bg.cpu().numpy())\n",
    "            \n",
    "            gt_parts_loc = torch.arange(landmarks_full.shape[1]).unsqueeze(0).repeat(landmarks_full.shape[0], 1)\n",
    "            visible = visible.detach().cpu()\n",
    "            gt_parts_loc = gt_parts_loc[visible]\n",
    "            all_nmi_gts.append(gt_parts_loc.cpu().numpy())\n",
    "\n",
    "\n",
    "    all_nmi_preds = np.concatenate(all_nmi_preds, axis=0)\n",
    "    all_nmi_preds_w_bg = np.concatenate(all_nmi_preds_w_bg, axis=0)\n",
    "    all_nmi_gts = np.concatenate(all_nmi_gts, axis=0)\n",
    "\n",
    "    # save kp\n",
    "    json_object = json.dumps(save_segment_kp_dict, indent=4)\n",
    "    with open(f\"cub_eval_kp.json\", \"w\") as out:\n",
    "        out.write(json_object)\n",
    "        \n",
    "    return all_nmi_gts, all_nmi_preds, all_nmi_preds_w_bg\n",
    "\n",
    "\n",
    "def eval_all(path=None, is_full=False, is_scops=False, num_parts=4, only_three=False, req_label=1, mask_type='sup', image_size=256, regress_landmark=True):\n",
    "    # define data transformation (no crop)\n",
    "    data_transforms = [transforms.Compose([\n",
    "        transforms.Resize(size=(image_size)),\n",
    "        To_tensor(),\n",
    "        transforms.Normalize(mean=(104.00698793, 116.66876762, 122.67891434), std=(1, 1, 1))\n",
    "    ]),\n",
    "        transforms.Compose([\n",
    "            transforms.Resize(size=(image_size)),\n",
    "            To_tensor()\n",
    "        ])]\n",
    "    # define dataset and loader\n",
    "    fit_data = CUB200(root=f'{dataset_root}', train=True, transform=data_transforms, \n",
    "                      resize=image_size, three_classes=only_three, mask_type=mask_type, crop_to_bbox=not is_full)\n",
    "    eval_data = CUB200(root=f'{dataset_root}', train=False, transform=data_transforms, \n",
    "                       resize=image_size, three_classes=only_three, req_label=req_label, \n",
    "                       mask_type=mask_type, crop_to_bbox=not is_full)\n",
    "    fit_loader = torch.utils.data.DataLoader(fit_data, batch_size=1, shuffle=False, num_workers=0, pin_memory=False, drop_last=False)\n",
    "    eval_loader = torch.utils.data.DataLoader(eval_data, batch_size=1, shuffle=False, num_workers=0, pin_memory=False, drop_last=False)\n",
    "    args2 = SimpleNamespace(dataset_root=dataset_root, use_lab=True, single_class=None,\n",
    "                            gpu=0, batch_size=6, input_size=image_size, split='test', model=\"DeepLab50_2branch\",\n",
    "                            num_parts=num_parts, num_classes=15, restore_from='None', unsup_mask=True)\n",
    "\n",
    "    args2.model = 'DeepLab50_2branch'\n",
    "    model = model_generator(args2, add_bg_mask=is_scops)\n",
    "    print(model.load_state_dict(torch.load(path) if is_scops else torch.load(path)[\"model_state_dict\"], strict=False))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    nmi_gts, nmi_preds, nmi_preds_w_bg = get_nmi_inputs(eval_loader, model, args2.num_parts, is_scops=is_scops)\n",
    "    print(np.unique(nmi_gts), np.unique(nmi_preds), np.unique(nmi_preds_w_bg))\n",
    "    if is_full:\n",
    "        nmi = normalized_mutual_info_score(nmi_gts, nmi_preds_w_bg) * 100\n",
    "        ari = adjusted_rand_score(nmi_gts, nmi_preds_w_bg) * 100\n",
    "    else:\n",
    "        nmi = normalized_mutual_info_score(nmi_gts, nmi_preds) * 100\n",
    "        ari = adjusted_rand_score(nmi_gts, nmi_preds) * 100\n",
    "    return nmi, ari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75db2032",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5794/5794 [04:12<00:00, 22.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14] [0 1 2 3] [0 1 2 3 4]\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5794/5794 [04:03<00:00, 23.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14] [0 1 2 3] [0 1 2 3 4]\n",
      "FG-NMI1: 46.06 FG-ARI1: 21.66 Full-NMI: 43.51 Full-ARI: 19.58\n"
     ]
    }
   ],
   "source": [
    "is_scops = False\n",
    "path = '../checkpoints/CUB/model_60000.pth'\n",
    "nmi1, ari1 = eval_all(path=path, is_full=False, is_scops=is_scops, only_three=False, req_label=None,regress_landmark=False)\n",
    "nmi2, ari2 = eval_all(path=path, is_full=True, is_scops=is_scops, only_three=False, req_label=None, regress_landmark=False)\n",
    "\n",
    "print(f'FG-NMI1: {nmi1:2.2f} FG-ARI1: {ari1:2.2f} Full-NMI: {nmi2:2.2f} Full-ARI: {ari2:2.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9367e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
